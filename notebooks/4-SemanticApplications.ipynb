{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment 4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxeWGHY9agKB"
      },
      "source": [
        "#Assignment 4\r\n",
        "Orazi Filippo,\r\n",
        "Rossolini Andrea "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMNPtUm-SOb1"
      },
      "source": [
        "# data and numerical management packages\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "# useful during debugging (progress bars)\r\n",
        "from tqdm import tqdm\r\n",
        "\r\n",
        "# Keras packages\r\n",
        "from keras import Sequential \r\n",
        "from keras.layers import Embedding, SimpleRNN, TimeDistributed, Dense, Bidirectional, Masking, LSTM, GRU, Input, Concatenate,Flatten, Add, Average, Dot, GlobalAveragePooling1D\r\n",
        "from keras import backend as K\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from keras.utils import to_categorical\r\n",
        "from keras import Model \r\n",
        "from keras.utils import plot_model\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "from sklearn.preprocessing import OneHotEncoder\r\n",
        "\r\n",
        "# utils\r\n",
        "from urllib import request\r\n",
        "import zipfile\r\n",
        "import gensim\r\n",
        "import gensim.downloader as gloader\r\n",
        "import scipy.sparse\r\n",
        "import tensorflow as tf\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import os\r\n",
        "import requests\r\n",
        "import zipfile\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BspxZcRjW0NG"
      },
      "source": [
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "def download_data(data_path):\n",
        "    toy_data_path = os.path.join(data_path, 'fever_data.zip')\n",
        "    toy_data_url_id = \"1wArZhF9_SHW17WKNGeLmX-QTYw9Zscl1\"\n",
        "    toy_url = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    if not os.path.exists(data_path):\n",
        "        os.makedirs(data_path)\n",
        "\n",
        "    if not os.path.exists(toy_data_path):\n",
        "        print(\"Downloading FEVER data splits...\")\n",
        "        with requests.Session() as current_session:\n",
        "            response = current_session.get(toy_url,\n",
        "                                   params={'id': toy_data_url_id},\n",
        "                                   stream=True)\n",
        "        save_response_content(response, toy_data_path)\n",
        "        print(\"Download completed!\")\n",
        "\n",
        "        print(\"Extracting dataset...\")\n",
        "        with zipfile.ZipFile(toy_data_path) as loaded_zip:\n",
        "            loaded_zip.extractall(data_path)\n",
        "        print(\"Extraction completed!\")\n",
        "\n",
        "download_data('dataset')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11mXWUdYWvOf"
      },
      "source": [
        "'''\n",
        "Deviding dataset in training, test and validation set\n",
        "'''\n",
        "path = \"./dataset/\"\n",
        "\n",
        "glob_vocabulary = set()\n",
        "glob_tags = set()\n",
        "bohs = set()\n",
        "\n",
        "train_set = pd.read_csv(path + \"train_pairs.csv\" )\n",
        "train_set.columns=[\"id\",\"claim\",\"evidence\",\"id_claim\",\"label\"]\n",
        "\n",
        "test_set = pd.read_csv(path + \"test_pairs.csv\" )\n",
        "test_set.columns=[\"id\",\"claim\",\"evidence\",\"id_claim\",\"label\"]\n",
        "\n",
        "val_set = pd.read_csv(path + \"val_pairs.csv\" )\n",
        "val_set.columns=[\"id\",\"claim\",\"evidence\",\"id_claim\",\"label\"]\n",
        "\n",
        "\n",
        "\n",
        "Y_train = train_set[\"label\"]\n",
        "X_train = train_set.drop([\"label\"], axis=1).drop([\"id\"], axis= 1)\n",
        "\n",
        "Y_val = val_set[\"label\"]\n",
        "X_val = val_set.drop([\"label\"], axis=1).drop([\"id\"], axis= 1)\n",
        "\n",
        "Y_test = test_set[\"label\"]\n",
        "X_test = test_set.drop([\"label\"], axis=1).drop([\"id\"], axis= 1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khX-NAK1wEYF"
      },
      "source": [
        "#Preprocessing \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TLTu0-2JQwi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "d458cfea-90cd-43a5-a5fa-1ef95baae8d5"
      },
      "source": [
        "import re\n",
        "from functools import reduce\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Config\n",
        "\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;-\\`\\'\\\"â€“]')\n",
        "GOOD_SYMBOLS_RE = re.compile('[^0-9a-z. \\t]')\n",
        "STOPWORDS = set([\"rrb\", \"lrb\", \"rsb\", \"lsb\", \"lcb\", \"rcb\"])\n",
        "\n",
        "def remove_number(text) :\n",
        "    \"\"\"\n",
        "    Removes the number and tab in front of the text\n",
        "    \"\"\"\n",
        "    pattern = r'[0-9]+?\\t'\n",
        "    return re.sub(pattern, '', text)\n",
        "\n",
        "def remove_hyperlinks(text) :\n",
        "    \"\"\"\n",
        "    Remove hyperlinks\n",
        "    \"\"\"\n",
        "    pattern = r'.\\t.*?$'\n",
        "    return re.sub(pattern, '.', text)\n",
        "\n",
        "def remove_pronunciations(text) :\n",
        "    \"\"\"\n",
        "    Remove the characters used to indicate a pronunciation\n",
        "    \"\"\"\n",
        "    pattern = r'-LSB-.*?-RSB-(\\s;)*?'\n",
        "    return re.sub(pattern, '', text)\n",
        "\n",
        "def split_periods(text) :\n",
        "    pattern = r'(\\s.+?)\\.'\n",
        "    return re.sub(pattern, r'\\1 .', text)\n",
        "\n",
        "def remove_comma_thousands(text) :\n",
        "    \"\"\"\n",
        "    Remove the comma used to indicate a number w/ more than three digits\n",
        "    \"\"\"\n",
        "    pattern = r'([0-9]{1,3}),([0-9]{1,3})'\n",
        "    text = re.sub(pattern, r'\\1\\2', text)\n",
        "    pattern = r'([0-9]{1,3}),'\n",
        "    return re.sub(pattern, r'\\1', text)\n",
        "\n",
        "def fix_date_merged(text) :\n",
        "    \"\"\"\n",
        "    fixes dates and days marged with other words\n",
        "    \"\"\"\n",
        "    pattern = r'([0-9]{1,4})([a-zA-Z]+?)'\n",
        "    return re.sub(pattern, r'\\1 \\2', text)\n",
        "\n",
        "def remove_repeated_ending_periods(text) :\n",
        "    \"\"\"\n",
        "    Remove words at the end of a period when these are repeated\n",
        "    \"\"\"\n",
        "    pattern = r'([a-zA-Z]{1,2}\\.)\\.$'\n",
        "    text = re.sub(pattern, '\\1 .', text)\n",
        "    pattern = r'\\.\\.$'\n",
        "    return re.sub(pattern, '.', text)\n",
        "\n",
        "def lower(text):\n",
        "    \"\"\"\n",
        "    Transforms given text to lower case.\n",
        "    Example:\n",
        "    Input: 'I really like New York city'\n",
        "    Output: 'i really like new your city'\n",
        "    \"\"\"\n",
        "    return text.lower()\n",
        "\n",
        "def replace_special_characters(text):\n",
        "    \"\"\"\n",
        "    Replaces special characters, such as paranthesis,\n",
        "    with spacing character\n",
        "    \"\"\"\n",
        "    return REPLACE_BY_SPACE_RE.sub(' ', text)\n",
        "\n",
        "def filter_out_uncommon_symbols(text):\n",
        "    \"\"\"\n",
        "    Removes any special character that is not in the\n",
        "    good symbols list (check regular expression)\n",
        "    \"\"\"\n",
        "    return GOOD_SYMBOLS_RE.sub('', text)\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    return ' '.join([x for x in text.split() if x and x not in STOPWORDS])\n",
        "\n",
        "def strip_text(text):\n",
        "    \"\"\"\n",
        "    Removes any left or right spacing (including carriage return) from text.\n",
        "    Example:\n",
        "    Input: '  This assignment is cool\\n'\n",
        "    Output: 'This assignment is cool'\n",
        "    \"\"\"\n",
        "    return text.strip()\n",
        "\n",
        "PREPROCESSING_PIPELINE = [\n",
        "                          remove_number,\n",
        "                          remove_hyperlinks,\n",
        "                          remove_pronunciations,\n",
        "                          split_periods,\n",
        "                          remove_comma_thousands,\n",
        "                          fix_date_merged,\n",
        "                          remove_repeated_ending_periods,\n",
        "                          lower,\n",
        "                          replace_special_characters,\n",
        "                          filter_out_uncommon_symbols,\n",
        "                          remove_stopwords,\n",
        "                          strip_text\n",
        "                          ]\n",
        "\n",
        "# Anchor method\n",
        "\n",
        "def text_prepare(text, filter_methods=None):\n",
        "    \"\"\"\n",
        "    Applies a list of pre-processing functions in sequence (reduce).\n",
        "    Note that the order is important here!\n",
        "    \"\"\"\n",
        "\n",
        "    filter_methods = filter_methods if filter_methods is not None else PREPROCESSING_PIPELINE\n",
        "\n",
        "    return reduce(lambda txt, f: f(txt), filter_methods, text)\n",
        "\n",
        "# Pre-processing\n",
        "\n",
        "print('Pre-processing text...')\n",
        "\n",
        "print(\"Preprocessing training_set\")\n",
        "train_set['evidence'] = train_set['evidence'].apply(lambda txt: text_prepare(txt))\n",
        "train_set['claim'] = train_set['claim'].apply(lambda txt: text_prepare(txt))\n",
        "print(\"Training_set done\")\n",
        "\n",
        "print(\"Preprocessing val_set\")\n",
        "val_set['evidence'] = val_set['evidence'].apply(lambda txt: text_prepare(txt))\n",
        "val_set['claim'] = val_set['claim'].apply(lambda txt: text_prepare(txt))\n",
        "print(\"Val_SET done\")\n",
        "\n",
        "print(\"Preprocessing test_set\")\n",
        "test_set['evidence'] = test_set['evidence'].apply(lambda txt: text_prepare(txt))\n",
        "test_set['claim'] = test_set['claim'].apply(lambda txt: text_prepare(txt))\n",
        "print(\"Test_set done\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"Pre-processing completed!\")\n",
        "train_set.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pre-processing text...\n",
            "Preprocessing training_set\n",
            "Training_set done\n",
            "Preprocessing val_set\n",
            "Val_SET done\n",
            "Preprocessing test_set\n",
            "Test_set done\n",
            "Pre-processing completed!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>claim</th>\n",
              "      <th>evidence</th>\n",
              "      <th>id_claim</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>chris hemsworth appeared in a perfect getaway .</td>\n",
              "      <td>hemsworth has also appeared in the science fic...</td>\n",
              "      <td>3</td>\n",
              "      <td>SUPPORTS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>roald dahl is a writer .</td>\n",
              "      <td>roald dahl 13 september 1916 23 november 1990 ...</td>\n",
              "      <td>7</td>\n",
              "      <td>SUPPORTS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>roald dahl is a governor .</td>\n",
              "      <td>roald dahl 13 september 1916 23 november 1990 ...</td>\n",
              "      <td>8</td>\n",
              "      <td>REFUTES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>ireland has relatively lowlying mountains .</td>\n",
              "      <td>the island s geography comprises relatively lo...</td>\n",
              "      <td>9</td>\n",
              "      <td>SUPPORTS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>ireland does not have relatively lowlying moun...</td>\n",
              "      <td>the island s geography comprises relatively lo...</td>\n",
              "      <td>10</td>\n",
              "      <td>REFUTES</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id                                              claim  ... id_claim     label\n",
              "0   0    chris hemsworth appeared in a perfect getaway .  ...        3  SUPPORTS\n",
              "1   1                           roald dahl is a writer .  ...        7  SUPPORTS\n",
              "2   2                         roald dahl is a governor .  ...        8   REFUTES\n",
              "3   3        ireland has relatively lowlying mountains .  ...        9  SUPPORTS\n",
              "4   4  ireland does not have relatively lowlying moun...  ...       10   REFUTES\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEz976y8xdJ-"
      },
      "source": [
        "#Tokenizazione"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgPJiPJIwEMr"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "import gensim.downloader as api\r\n",
        "from nltk.tokenize.treebank import TreebankWordTokenizer\r\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\r\n",
        "\r\n",
        "EMB_DIM = 50\r\n",
        "EMBEDDING_SIZE = 50\r\n",
        "embedding_model = api.load(f\"glove-wiki-gigaword-{EMB_DIM}\")\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwfSWutZz442"
      },
      "source": [
        "def tokenizer(data_set):\r\n",
        "  toke = pd.DataFrame(columns=[\"claim\", \"evidence\", \"label\"])\r\n",
        "  toke[\"claim\"] = data_set[\"claim\"].apply(ToktokTokenizer().tokenize)\r\n",
        "  toke[\"evidence\"] = data_set[\"evidence\"].apply(ToktokTokenizer().tokenize)\r\n",
        "  toke[\"label\"] = data_set[\"label\"]\r\n",
        "  return toke\r\n",
        "\r\n",
        "train_toke = tokenizer(train_set)\r\n",
        "val_toke = tokenizer(val_set)\r\n",
        "test_toke = tokenizer(test_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR_qmmk-616a"
      },
      "source": [
        "##Building vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZli74q50vuF"
      },
      "source": [
        "voc = set(embedding_model.vocab.keys())\r\n",
        "\r\n",
        "def build_vocab(df) :\r\n",
        "  res = []\r\n",
        "  for col in [\"claim\", \"evidence\"] :\r\n",
        "    for r in df[col] :\r\n",
        "      res += r\r\n",
        "  res = pd.unique(res)\r\n",
        "  return res\r\n",
        "\r\n",
        "train_voc = np.array(build_vocab(train_toke), dtype=str)   \r\n",
        "voc.update(train_voc)\r\n",
        "val_voc = np.array(build_vocab(val_toke), dtype=str)   \r\n",
        "voc.update(val_voc)\r\n",
        "test_voc = np.array(build_vocab(test_toke), dtype=str)   \r\n",
        "voc.update(test_voc)\r\n",
        "\r\n",
        "vocab_len = len(voc)\r\n",
        "word_to_index = dict(zip(voc, range(1, len(voc)+1))) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ao37GhY66Gp"
      },
      "source": [
        "##Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xJ8qR1D3hm8"
      },
      "source": [
        "def encode(toke, word_to_idx) :\r\n",
        "  encoded = pd.DataFrame(columns=[\"claim\", \"evidence\", \"label\"])\r\n",
        "  encoded[\"claim\"] = toke[\"claim\"].apply(lambda s: [word_to_idx[w] for w in s])\r\n",
        "  encoded[\"evidence\"] = toke[\"evidence\"].apply(lambda s:[word_to_idx[w] for w in s])\r\n",
        "  encoded[\"label\"] = toke[\"label\"].apply(lambda x: 1 if x==\"SUPPORTS\" else 0)\r\n",
        "  return encoded\r\n",
        "\r\n",
        "train_encoded = encode(train_toke, word_to_index)\r\n",
        "val_encoded = encode(val_toke, word_to_index)\r\n",
        "test_encoded = encode(test_toke, word_to_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTFIvpX7neRk",
        "outputId": "0ab5f5a3-850d-410c-d6d8-d6caebaf2fbd"
      },
      "source": [
        "train_encoded[\"claim\"], train_encoded[\"evidence\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0         [304403, 172010, 110904, 310387, 26881, 211718...\n",
              " 1              [73008, 75225, 283905, 26881, 136216, 76359]\n",
              " 2              [73008, 75225, 283905, 26881, 317431, 76359]\n",
              " 3             [403641, 355055, 296257, 394446, 3386, 76359]\n",
              " 4         [403641, 94455, 282144, 151502, 296257, 394446...\n",
              "                                 ...                        \n",
              " 121735    [141014, 158490, 334874, 7879, 21017, 78309, 1...\n",
              " 121736    [21017, 78309, 283905, 231567, 171025, 206507,...\n",
              " 121737    [21017, 78309, 283905, 394042, 309007, 192518,...\n",
              " 121738    [21017, 78309, 158490, 342203, 211434, 334874,...\n",
              " 121739    [21017, 78309, 158490, 342203, 211434, 334874,...\n",
              " Name: claim, Length: 121740, dtype: object,\n",
              " 0         [172010, 355055, 380225, 110904, 310387, 33487...\n",
              " 1         [73008, 75225, 352357, 112469, 14105, 389836, ...\n",
              " 2         [73008, 75225, 352357, 112469, 14105, 389836, ...\n",
              " 3         [334874, 269566, 52496, 176056, 275589, 296257...\n",
              " 4         [334874, 269566, 52496, 176056, 275589, 296257...\n",
              "                                 ...                        \n",
              " 121735    [21017, 101950, 78309, 342203, 141014, 10799, ...\n",
              " 121736    [21017, 101950, 78309, 342203, 141014, 10799, ...\n",
              " 121737    [21017, 101950, 78309, 342203, 141014, 10799, ...\n",
              " 121738    [21017, 101950, 78309, 342203, 141014, 10799, ...\n",
              " 121739    [21017, 101950, 78309, 342203, 141014, 10799, ...\n",
              " Name: evidence, Length: 121740, dtype: object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtLHNZnR7Aua"
      },
      "source": [
        "##Padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IV07xgtF4Zo0"
      },
      "source": [
        "MAX_SEQ_LENGTH_CLAIM = np.int(np.max([len(a) for a in train_encoded[\"claim\"]]))\r\n",
        "MAX_SEQ_LENGTH_EVIDENCE = np.int(np.max([len(a) for a in train_encoded[\"evidence\"]]))\r\n",
        "\r\n",
        "'''claim'''\r\n",
        "X_train_claim_padded = pad_sequences(train_encoded[\"claim\"], maxlen=MAX_SEQ_LENGTH_CLAIM, padding=\"post\", truncating=\"post\")\r\n",
        "X_val_claim_padded = pad_sequences(val_encoded[\"claim\"], maxlen=MAX_SEQ_LENGTH_CLAIM, padding=\"post\", truncating=\"post\")\r\n",
        "X_test_claim_padded = pad_sequences(test_encoded[\"claim\"], maxlen=MAX_SEQ_LENGTH_CLAIM, padding=\"post\", truncating=\"post\")\r\n",
        "\r\n",
        "'''evidence'''\r\n",
        "X_train_evidence_padded = pad_sequences(train_encoded[\"evidence\"], maxlen=MAX_SEQ_LENGTH_EVIDENCE, padding=\"post\", truncating=\"post\")\r\n",
        "X_val_evidence_padded = pad_sequences(val_encoded[\"evidence\"], maxlen=MAX_SEQ_LENGTH_EVIDENCE, padding=\"post\", truncating=\"post\")\r\n",
        "X_test_evidence_padded = pad_sequences(test_encoded[\"evidence\"], maxlen=MAX_SEQ_LENGTH_EVIDENCE, padding=\"post\", truncating=\"post\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9-mJL3N3cH2"
      },
      "source": [
        "encoded_Y_train = train_encoded[\"label\"]\r\n",
        "encoded_Y_val = val_encoded[\"label\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JMJPfkG8Pyo"
      },
      "source": [
        "##Building embedding matrix\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gy-IAXF18Szj",
        "outputId": "17ac3aa5-5849-4354-f49c-67df15276a60"
      },
      "source": [
        "emmb_mat =  [np.zeros((EMB_DIM))]\r\n",
        "\r\n",
        "for w in voc :\r\n",
        "  if w in embedding_model.vocab :\r\n",
        "    emmb_mat.append(embedding_model[w])\r\n",
        "  else:\r\n",
        "    emmb_mat.append(np.random.uniform(low=-1.0, high=1.0, size=EMB_DIM))\r\n",
        "emmb_mat = np.array(emmb_mat)\r\n",
        "emmb_mat.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(405252, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PK9to2l9E-P"
      },
      "source": [
        "#NN building blocks\r\n",
        "In the following section we are going to define all the layers and the functions needed to build the different models.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8vsUD7x9k46"
      },
      "source": [
        "##Inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33norpYu9Knl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5465ee7d-c73a-4387-a159-675a4cda8bf8"
      },
      "source": [
        "input_claim = Input(shape=(MAX_SEQ_LENGTH_CLAIM), name='input_claim', dtype=tf.int32)\r\n",
        "input_evidence = Input(shape=(MAX_SEQ_LENGTH_EVIDENCE), name='input_evidence', dtype=tf.int32)\r\n",
        "MAX_SEQ_LENGTH_CLAIM, MAX_SEQ_LENGTH_EVIDENCE"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(68, 120)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d0ABP9x9mN6"
      },
      "source": [
        "##Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UCKhWFi9UTH"
      },
      "source": [
        "def embedding_layer_getter(input_layer_claim, input_layer_evidence, embedding_matrix):\r\n",
        "  \"\"\"\r\n",
        "  Returns two embedding layers, one for claim and one for evidence\r\n",
        "  \"\"\"\r\n",
        "  layer_emb = Embedding(\r\n",
        "        embedding_matrix.shape[0],    # vocab size \r\n",
        "        embedding_matrix.shape[1],    # embedding dimension\r\n",
        "        weights = [embedding_matrix],\r\n",
        "        mask_zero = True,\r\n",
        "        name = \"Embedding_layer\",\r\n",
        "        trainable = False\r\n",
        "    )\r\n",
        "  claim_emb = layer_emb(input_layer_claim)\r\n",
        "  evidence_emb = layer_emb(input_layer_evidence)\r\n",
        "  return (claim_emb, evidence_emb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT5hrAdn9kUu"
      },
      "source": [
        "##Sentence embedding\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-0sX46Q9ubR"
      },
      "source": [
        "def RNN_layer_monodirectional(claim_emb, evidence_emb):\r\n",
        "  \"\"\"\r\n",
        "  Returns two layers with a monodirectional LSTM\r\n",
        "  \"\"\"\r\n",
        "  claim_output, claim_sentence_emb, _ = LSTM(64, dropout = 0.2,  return_sequences=True, return_state=True)(claim_emb)\r\n",
        "  evidence_output, evidence_sentence_emb, _ = LSTM(64, dropout = 0.2,  return_sequences=True, return_state= True)(evidence_emb)\r\n",
        "  return (claim_sentence_emb, evidence_sentence_emb)\r\n",
        "\r\n",
        "def RNN_layer_bidirectional(claim_emb, evidence_emb):\r\n",
        "  \"\"\"\r\n",
        "  Returns two layers with a bidirectional LSTM\r\n",
        "   - concatenates claim_forward + claim_backward\r\n",
        "   - concatenates evidence_forward + evidence_backward\r\n",
        "  \"\"\"\r\n",
        "  claim_output, claim_forward, _, claim_backward, _  = Bidirectional(LSTM(64, dropout = 0.2,  return_sequences=True, return_state=True))(claim_emb)\r\n",
        "  evidence_output, evidence_forward, _ , evidence_backward, _ = Bidirectional(LSTM(64, dropout = 0.2,  return_sequences=True, return_state= True))(evidence_emb)\r\n",
        "  claim_sentence_emb = Concatenate()([claim_forward, claim_backward])\r\n",
        "  evidence_sentence_emb = Concatenate()([evidence_forward, evidence_backward])\r\n",
        "  return (claim_sentence_emb, evidence_sentence_emb)\r\n",
        "\r\n",
        "def RNN_layer_bidirectional_average(claim_emb, evidence_emb):\r\n",
        "  \"\"\"\r\n",
        "  Returns two layers with a bidirectional LSTM\r\n",
        "   - concatenates claim_forward + claim_backward through a GlobalAveragePooling\r\n",
        "   - concatenates evidence_forward + evidence_backward through a GlobalAveragePooling\r\n",
        "  \"\"\"\r\n",
        "  claim_output = Bidirectional(LSTM(64, dropout = 0.2,  return_sequences=True))(claim_emb)\r\n",
        "  evidence_output = Bidirectional(LSTM(64, dropout = 0.2,  return_sequences=True))(evidence_emb)\r\n",
        "  claim_pooled = GlobalAveragePooling1D()(claim_output)\r\n",
        "  evidence_pooled = GlobalAveragePooling1D()(evidence_output)\r\n",
        "  return (claim_pooled, evidence_pooled)\r\n",
        "\r\n",
        "def dot_layer_cosine_similarity(claim_emb, evidence_emb):\r\n",
        "  \"\"\"\r\n",
        "  Returns a layer that computes the cosine similarity\r\n",
        "  \"\"\"\r\n",
        "  return Dot(axes=1, normalize=True)([claim_emb, evidence_emb])\r\n",
        "\r\n",
        "def bov_layer(claim_emb, evidence_emb):\r\n",
        "  return GlobalAveragePooling1D()(claim_emb), GlobalAveragePooling1D()(evidence_emb)\r\n",
        "\r\n",
        "def mlp_layer(embedding_layer):\r\n",
        "  flattened = Flatten()(embedding_layer)\r\n",
        "  d_1 = Dense(512)(flattened)\r\n",
        "  d_2 = Dense(256)(d_1)\r\n",
        "  d_3 = Dense(128)(d_2)\r\n",
        "  d_4 = Dense(64)(d_3)\r\n",
        "  return d_4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0XaFwPh-KbC"
      },
      "source": [
        "##Merging sentence embeddings\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7K7wqvY9v0D"
      },
      "source": [
        "def input_concatenate(claim_sentence_emb, evidence_sentence_emb):\r\n",
        "  \"\"\"\r\n",
        "  concatenate the two forward output to obtain a comprehensive claim + evidence sentence embedding\r\n",
        "  \"\"\"\r\n",
        "  return Concatenate()([claim_sentence_emb, evidence_sentence_emb])\r\n",
        "\r\n",
        "def input_sum(claim_forward, evidence_forward):\r\n",
        "  \"\"\"\r\n",
        "  concatenate the two forward output to obtain a comprehensive claim + evidence sentence embedding\r\n",
        "  \"\"\"\r\n",
        "  return Add()([claim_forward, evidence_forward])\r\n",
        "\r\n",
        "def input_avg(claim_forward, evidence_forward):\r\n",
        "  \"\"\"\r\n",
        "  average the previoous layerd\r\n",
        "  \"\"\"\r\n",
        "  return Average()([claim_forward, evidence_forward])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVEu51s5-V75"
      },
      "source": [
        "##Binary classification Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oTib1CZ-Y6M"
      },
      "source": [
        "def binary_classification_layer(layer_pre):\r\n",
        "    return Dense(1, activation=\"sigmoid\")(\r\n",
        "        Dense(8,activation=\"relu\")(\r\n",
        "            Dense(16, activation=\"relu\")(\r\n",
        "                Dense(32, activation=\"relu\")(\r\n",
        "                    Dense(64, activation=\"relu\")(\r\n",
        "                        Dense(128, activation=\"relu\")(layer_pre))))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y6X-O8K-f2L"
      },
      "source": [
        "#Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJoopiDk0At1"
      },
      "source": [
        "def get_model(input_claim, input_evidence, emmb_mat, s_emb = \"bov\", merge = \"conc\", cosine = False):\r\n",
        "  embedding_layer = embedding_layer_getter(input_claim, input_evidence, emmb_mat)\r\n",
        "  '''\r\n",
        "  single sentence embedding\r\n",
        "  '''\r\n",
        "  if s_emb == \"RNN\":\r\n",
        "    sentence_emb = RNN_layer_bidirectional(*embedding_layer)\r\n",
        "  elif s_emb == \"avg\":\r\n",
        "    sentence_emb = RNN_layer_bidirectional_average(*embedding_layer)\r\n",
        "  elif s_emb == \"bov\":\r\n",
        "    sentence_emb = bov_layer(*embedding_layer)\r\n",
        "  elif s_emb == \"MLP\":\r\n",
        "    sentence_emb = (mlp_layer(embedding_layer[0]), mlp_layer(embedding_layer[1]))  \r\n",
        "  '''\r\n",
        "  sentences merge\r\n",
        "  '''\r\n",
        "  if merge == \"conc\":\r\n",
        "    merged_input = input_concatenate(*sentence_emb)\r\n",
        "  elif merge == \"sum\":\r\n",
        "    merged_input = input_sum(*sentence_emb)\r\n",
        "  elif merge == \"avg\":\r\n",
        "    merged_input = input_avg(*sentence_emb)\r\n",
        "\r\n",
        "  '''\r\n",
        "  cosine\r\n",
        "  '''\r\n",
        "  if cosine:\r\n",
        "    cosine = dot_layer_cosine_similarity(*sentence_emb)\r\n",
        "    merged_input = input_concatenate(merged_input, cosine)\r\n",
        "\r\n",
        "  model_layers = binary_classification_layer(merged_input)\r\n",
        "  return Model(inputs = [input_claim, input_evidence], \r\n",
        "            outputs= [model_layers])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kx2nSQFJQgi4",
        "outputId": "16bedc18-8560-45c6-d359-c18b63f8deda"
      },
      "source": [
        "s_embs = [\"RNN\",\"avg\",\"bov\",\"MLP\"]\r\n",
        "merges = [\"conc\", \"sum\", \"avg\"]\r\n",
        "\r\n",
        "print(\"Correctness test\")\r\n",
        "for s_emb in s_embs:\r\n",
        "  for merge in merges:\r\n",
        "    model_tmp = get_model(input_claim, input_evidence, emmb_mat, s_emb, merge, cosine = False)\r\n",
        "    model_tmp = get_model(input_claim, input_evidence, emmb_mat, s_emb, merge, cosine = True)\r\n",
        "print(\"Perfectly working\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correctness test\n",
            "Perfectly working\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1VFgj0WRY9C",
        "outputId": "e73e85cb-5269-4006-baf2-da8b999c3af9"
      },
      "source": [
        "model_1 = get_model(input_claim, input_evidence, emmb_mat,s_emb=\"avg\", cosine= True)\r\n",
        "plot_model(model_1, show_shapes=True)\r\n",
        "model_1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\r\n",
        "history = model_1.fit([X_train_claim_padded,X_train_evidence_padded], encoded_Y_train, batch_size=100, epochs=15, validation_data=([X_val_claim_padded, X_val_evidence_padded], encoded_Y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "1218/1218 [==============================] - 558s 445ms/step - loss: 0.5249 - accuracy: 0.7572 - val_loss: 0.5958 - val_accuracy: 0.6635\n",
            "Epoch 2/15\n",
            "1218/1218 [==============================] - 536s 440ms/step - loss: 0.4324 - accuracy: 0.8162 - val_loss: 0.6099 - val_accuracy: 0.6819\n",
            "Epoch 3/15\n",
            "1218/1218 [==============================] - 538s 441ms/step - loss: 0.4013 - accuracy: 0.8292 - val_loss: 0.5616 - val_accuracy: 0.6910\n",
            "Epoch 4/15\n",
            "1218/1218 [==============================] - 533s 438ms/step - loss: 0.3845 - accuracy: 0.8372 - val_loss: 0.5275 - val_accuracy: 0.7168\n",
            "Epoch 5/15\n",
            "1218/1218 [==============================] - 529s 434ms/step - loss: 0.3662 - accuracy: 0.8437 - val_loss: 0.5757 - val_accuracy: 0.7090\n",
            "Epoch 6/15\n",
            "1218/1218 [==============================] - 533s 437ms/step - loss: 0.3521 - accuracy: 0.8503 - val_loss: 0.5368 - val_accuracy: 0.7253\n",
            "Epoch 7/15\n",
            "1218/1218 [==============================] - 538s 441ms/step - loss: 0.3421 - accuracy: 0.8555 - val_loss: 0.5533 - val_accuracy: 0.7281\n",
            "Epoch 8/15\n",
            "1218/1218 [==============================] - 536s 440ms/step - loss: 0.3330 - accuracy: 0.8573 - val_loss: 0.6056 - val_accuracy: 0.7209\n",
            "Epoch 9/15\n",
            "1218/1218 [==============================] - 530s 435ms/step - loss: 0.3236 - accuracy: 0.8633 - val_loss: 0.5052 - val_accuracy: 0.7538\n",
            "Epoch 10/15\n",
            "1218/1218 [==============================] - 535s 439ms/step - loss: 0.3146 - accuracy: 0.8676 - val_loss: 0.5511 - val_accuracy: 0.7372\n",
            "Epoch 11/15\n",
            "1218/1218 [==============================] - 537s 441ms/step - loss: 0.3060 - accuracy: 0.8710 - val_loss: 0.5475 - val_accuracy: 0.7269\n",
            "Epoch 12/15\n",
            "1218/1218 [==============================] - 535s 439ms/step - loss: 0.2998 - accuracy: 0.8734 - val_loss: 0.5347 - val_accuracy: 0.7397\n",
            "Epoch 13/15\n",
            "1218/1218 [==============================] - 539s 442ms/step - loss: 0.2934 - accuracy: 0.8759 - val_loss: 0.5650 - val_accuracy: 0.7424\n",
            "Epoch 14/15\n",
            "1218/1218 [==============================] - 528s 434ms/step - loss: 0.2873 - accuracy: 0.8781 - val_loss: 0.5166 - val_accuracy: 0.7464\n",
            "Epoch 15/15\n",
            "1218/1218 [==============================] - 540s 443ms/step - loss: 0.2838 - accuracy: 0.8794 - val_loss: 0.5813 - val_accuracy: 0.7478\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwZhjdvGzjHp"
      },
      "source": [
        "# Performance evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nk8rT5zYz5Sl"
      },
      "source": [
        "def get_caim_from_prediction(pred) :\r\n",
        "  pred_claim = pd.DataFrame()\r\n",
        "  pred_claim[\"id_claim\"] =val_set[\"id_claim\"]\r\n",
        "  pred_claim[\"label\"] = np.rint(pred)\r\n",
        "  return pred_claim\r\n",
        "\r\n",
        "def majority_evaluation(pred_claim) :\r\n",
        "  cl_list = pd.DataFrame(columns=[\"id_claim\", \"result\"])\r\n",
        "  cl_list[\"id_claim\"] = pd.unique(pred_claim[\"id_claim\"])\r\n",
        "  for id_claim in cl_list[\"id_claim\"]:\r\n",
        "    labels = pred_claim[pred_claim[\"id_claim\"] == id_claim]\r\n",
        "    supports = len(labels[labels[\"label\"] == 1])\r\n",
        "    refutes = len(labels[labels[\"label\"] == 0])\r\n",
        "    if supports > refutes :\r\n",
        "      cl_list[\"result\"][cl_list[\"id_claim\"] == id_claim] = 1\r\n",
        "    else :\r\n",
        "      cl_list[\"result\"][cl_list[\"id_claim\"] == id_claim] = 0\r\n",
        "  return cl_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tso0c-XP_GQV"
      },
      "source": [
        "'''\r\n",
        "We disabled some warnings that don't affect the operation\r\n",
        "'''\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings('ignore')\r\n",
        "\r\n",
        "predictions = model_1.predict([X_val_claim_padded, X_val_evidence_padded])\r\n",
        "pred = get_caim_from_prediction(predictions)\r\n",
        "majority = majority_evaluation(pred)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}