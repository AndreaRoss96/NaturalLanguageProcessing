{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia di Assignment_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NX47_zn5QzLo"
      },
      "source": [
        "# Orazi Filippo, Rossolini Andrea"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udwVKBiQBXRv"
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdLwx8TpChIi"
      },
      "source": [
        "#! git clone https://github.com/keras-team/keras-contrib\n",
        "#% cd keras-contrib/\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaSwE8MoKdq3"
      },
      "source": [
        "# system packages\n",
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "\n",
        "# data and numerical management packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# useful during debugging (progress bars)\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Keras packages\n",
        "from keras import Sequential \n",
        "from keras.layers import Embedding, SimpleRNN, TimeDistributed, Dense, Bidirectional, Masking, LSTM, GRU\n",
        "from keras import backend as K\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# utils\n",
        "from urllib import request\n",
        "import zipfile\n",
        "import gensim\n",
        "import gensim.downloader as gloader\n",
        "import scipy.sparse\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvq1793DSGqP"
      },
      "source": [
        "! pip install tf2crf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4_wqPdlBcKS"
      },
      "source": [
        "# Assignment 3 : Sequence labelling with RNNs\n",
        "In this assignement we will ask you to perform POS tagging.\n",
        "\n",
        "You are asked to follow these steps:\n",
        "*   Download the corpora and split it in training and test sets, structuring a dataframe.\n",
        "*   Embed the words using GloVe embeddings\n",
        "*   Create a baseline model, using a simple neural architecture\n",
        "*   Experiment doing small modifications to the model\n",
        "*   Evaluate your best model\n",
        "*   Analyze the errors of your model\n",
        "\n",
        "**Corpora**:\n",
        "Ignore the numeric value in the third column, use only the words/symbols and its label.\n",
        "https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip \n",
        "\n",
        "**Splits**: documents 1-100 are the train set, 101-150 validation set, 151-199 test set.\n",
        "\n",
        "**Baseline**: two layers architecture: a Bidirectional LSTM and a Dense/Fully-Connected layer on top.\n",
        "\n",
        "**Modifications**: experiment using a GRU instead of the LSTM, adding an additional LSTM layer, and using a CRF in addition to the LSTM. Each of this change must be done by itself (don't mix these modifications).\n",
        "\n",
        "**Training and Experiments**: all the experiments must involve only the training and validation sets.\n",
        "\n",
        "**Evaluation**: in the end, only the best model of your choice must be evaluated on the test set. The main metric must be F1-Macro computed between the various part of speech (without considering punctuation classes).\n",
        "\n",
        "**Error Analysis** (optional) : analyze the errors done by your model, try to understand which may be the causes and think about how to improve it.\n",
        "\n",
        "**Report**: You are asked to deliver a small report of about 4-5 lines in the .txt file that sums up your findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_BgimE4No6Q"
      },
      "source": [
        "# Download the corpora and split it in training and test sets, structuring a dataframe.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbTaZG72NnGI"
      },
      "source": [
        "# Config\n",
        "print(\"Current work directory: {}\".format(os.getcwd()))\n",
        "\n",
        "dataset_folder = os.path.join(os.getcwd(), \"Datasets\")\n",
        "\n",
        "if not os.path.exists(dataset_folder):\n",
        "    os.makedirs(dataset_folder)\n",
        "\n",
        "dataset_path = os.path.join(dataset_folder, \"Movies.tar.gz\")\n",
        "\n",
        "print(dataset_path)\n",
        "\n",
        "def download_dataset(download_path, url):\n",
        "    if not os.path.exists(download_path):\n",
        "        print(\"Downloading dataset...\")\n",
        "        request.urlretrieve(url, download_path)\n",
        "        print(\"Download complete!\")\n",
        "\n",
        "def extract_dataset(download_path, extract_path):\n",
        "    print(\"Extracting dataset... (it may take a while...)\")\n",
        "    with zipfile.ZipFile(download_path, 'r') as zip_ref:\n",
        "      zip_ref.extractall(extract_path)\n",
        "    print(\"Extraction completed!\")\n",
        "\n",
        "# Download\n",
        "download_dataset(dataset_path, url)\n",
        "\n",
        "# Extraction\n",
        "extract_dataset(dataset_path, dataset_folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISuPQDmEq6Ww"
      },
      "source": [
        "print(dataset_folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_86aT1_Omri"
      },
      "source": [
        "##Dividing documents into sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11mXWUdYWvOf"
      },
      "source": [
        "'''\n",
        "Deviding dataset in training, test and validation set\n",
        "'''\n",
        "path = dataset_folder + \"/dependency_treebank/\"\n",
        "\n",
        "glob_vocabulary = set()\n",
        "glob_tags = set()\n",
        "bohs = set()\n",
        "\n",
        "dataframe = {\n",
        "    \"sentences\" : [],\n",
        "    \"tags\" : [],\n",
        "    \"split\" : []\n",
        "} # array of dictionaries\n",
        "\n",
        "for count, document in enumerate(os.scandir(path)):\n",
        "  if count <= 101 :\n",
        "    split = \"train\"\n",
        "  elif 101 < count <= 151:\n",
        "    split = \"test\"\n",
        "  else :\n",
        "    split = \"val\"\n",
        "  with open(document, 'r') as file:\n",
        "    sentence = [line.rstrip() for line in file.readlines()]\n",
        "    sentence_words = []\n",
        "    sentence_tags = []\n",
        "    for word in sentence:\n",
        "      if word.strip():\n",
        "        token, tag, _ = word.rstrip().split(\"\\t\")\n",
        "        if token == '.':\n",
        "          dataframe[\"sentences\"].append(sentence_words)\n",
        "          dataframe[\"tags\"].append(sentence_tags)\n",
        "          dataframe[\"split\"].append(split)\n",
        "          sentence_words = []\n",
        "          sentence_tags = []  \n",
        "          continue\n",
        "        glob_vocabulary.add(token)\n",
        "        glob_tags.add(tag)\n",
        "\n",
        "        sentence_words.append(token)\n",
        "        sentence_tags.append(tag)\n",
        "        \n",
        "    dataframe[\"sentences\"].append(sentence_words)\n",
        "    dataframe[\"tags\"].append(sentence_tags)\n",
        "    dataframe[\"split\"].append(split)\n",
        "\n",
        "df = pd.DataFrame(dataframe)\n",
        "print(df.shape)\n",
        "print(df[\"sentences\"][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdwjD23dPEXK"
      },
      "source": [
        "##Dividing dataset into Train, Validation and Test sets "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "526I-XB2WFVB"
      },
      "source": [
        "X_train = df[df[\"split\"] == \"train\"][\"sentences\"].copy()\n",
        "Y_train = df[df[\"split\"] == \"train\"][\"tags\"].copy()\n",
        "\n",
        "X_val = df[df[\"split\"] == \"val\"][\"sentences\"].copy()\n",
        "Y_val = df[df[\"split\"] == \"val\"][\"tags\"].copy()\n",
        "\n",
        "X_test = df[df[\"split\"] == \"test\"][\"sentences\"].copy()\n",
        "Y_test = df[df[\"split\"] == \"test\"][\"tags\"].copy()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v23YwAwQO7Ar"
      },
      "source": [
        "#Building Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KucZ613liRaY"
      },
      "source": [
        "def build_vocabulary(sentences):\n",
        "    \"\"\"\n",
        "    Given a dataset, builds the corresponding word vocabulary.\n",
        "\n",
        "    :param df: dataset from which we want to build the word vocabulary (pandas.DataFrame)\n",
        "    :return:\n",
        "      - word vocabulary: vocabulary index to word\n",
        "      - inverse word vocabulary: word to vocabulary index\n",
        "      - word listing: set of unique terms that build up the vocabulary\n",
        "    \"\"\"\n",
        "    t = Tokenizer(filters=\"\", lower=False)\n",
        "    t.fit_on_texts(sentences)\n",
        "    word_to_idx = dict(zip(t.word_index.keys(), [a for a in t.word_index.values()])) \n",
        "    idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "    return idx_to_word, word_to_idx, list(word_to_idx.keys()), t\n",
        "\n",
        "MAX_SEQ_LENGTH = np.int(np.max([len(a) for a in X_train]))\n",
        "EMBEDDING_SIZE  = 50  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNQWwRSoPbiw"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWIaJJJzfuim"
      },
      "source": [
        "# Embed the words using GloVe embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpzDkIBzPhox"
      },
      "source": [
        "##Loading the embedding model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1bRAzZwh0YI"
      },
      "source": [
        "def load_embedding_model(model_type, embedding_dimension=50):\n",
        "    \"\"\"\n",
        "    Loads a pre-trained word embedding model via gensim library.\n",
        "\n",
        "    :param model_type: name of the word embedding model to load.\n",
        "    :param embedding_dimension: size of the embedding space to consider\n",
        "\n",
        "    :return\n",
        "        - pre-trained word embedding model (gensim KeyedVectors object)\n",
        "    \"\"\"\n",
        "\n",
        "    download_path = \"\"\n",
        "\n",
        "    # Find the correct embedding model name\n",
        "    if model_type.strip().lower() == 'word2vec':\n",
        "        download_path = \"word2vec-google-news-300\"\n",
        "\n",
        "    elif model_type.strip().lower() == 'glove':\n",
        "        download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
        "\n",
        "    else:\n",
        "        raise AttributeError(\"Unsupported embedding model type! Available ones: word2vec, glove\")\n",
        "\n",
        "    # Check download\n",
        "    try:\n",
        "        emb_model = gloader.load(download_path)\n",
        "    except ValueError as e:\n",
        "        print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
        "        print(\"Word2Vec: 300\")\n",
        "        print(\"Glove: 50, 100, 200, 300\")\n",
        "        raise e\n",
        "\n",
        "    return emb_model\n",
        "\n",
        "\n",
        "# Modify these variables as you wish!\n",
        "# Glove -> 50, 100, 200, 300\n",
        "# Word2Vec -> 300\n",
        "embedding_model_type = \"glove\"\n",
        "embedding_dimension = 50\n",
        "\n",
        "embedding_model = load_embedding_model(embedding_model_type, embedding_dimension)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T08pM6g9y3Uy"
      },
      "source": [
        "## handling OOV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bm7UIWG-0ukw"
      },
      "source": [
        "def co_occurrence_count(df, idx_to_word, word_to_idx, window_size=4):\n",
        "    \"\"\"\n",
        "    Builds word-word co-occurrence matrix based on word counts.\n",
        "\n",
        "    :param df: pre-processed dataset (pandas.DataFrame)\n",
        "    :param idx_to_word: vocabulary map (index -> word) (dict)\n",
        "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
        "\n",
        "    :return\n",
        "      - co_occurrence symmetric matrix of size |V| x |V| (|V| = vocabulary size)\n",
        "    \"\"\"\n",
        "    lil = scipy.sparse.lil_matrix((len(word_to_idx)+1, len(word_to_idx)+1))\n",
        "    for count,line in enumerate(df):\n",
        "      lane_to_idx = []\n",
        "      for v in line:\n",
        "        # this allows the handling of OOV \n",
        "        if v not in word_to_idx:\n",
        "          continue\n",
        "        lane_to_idx.append(word_to_idx[v])\n",
        "      for index, word in enumerate(lane_to_idx):\n",
        "          for i in range(1, int(window_size)+1):\n",
        "            if index - i >= 0:\n",
        "                lil[word,lane_to_idx[index - i ]] -= -1\n",
        "            if index + i < len(lane_to_idx):\n",
        "                a = lane_to_idx[index + i ]\n",
        "                lil[word, a] -= -1\n",
        "    return lil"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Q5ANaeSy-We"
      },
      "source": [
        "def check_OOV_terms(embedding_model, word_listing):\n",
        "    \"\"\"\n",
        "    Checks differences between pre-trained embedding model vocabulary\n",
        "    and dataset specific vocabulary in order to highlight out-of-vocabulary terms.\n",
        "\n",
        "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
        "    :param word_listing: dataset specific vocabulary (list)\n",
        "\n",
        "    :return\n",
        "        - list of OOV terms\n",
        "    \"\"\"\n",
        "    return list(filter(lambda term : term not in embedding_model, word_listing))\n",
        "\n",
        "oov_terms = check_OOV_terms(embedding_model, glob_vocabulary)\n",
        "\n",
        "print(\"Total OOV terms: {0} ({1:.2f}%)\".format(len(oov_terms), float(len(oov_terms)) / len(glob_vocabulary)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XC1F_44lJUDF"
      },
      "source": [
        "def build_embedding_matrix(embedding_model, embedding_dimension, word_to_idx, idx_to_word, oov_terms, co_occurrence_count_matrix):\n",
        "    \"\"\"\n",
        "    Builds the embedding matrix of a specific dataset given a pre-trained word embedding model\n",
        "\n",
        "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
        "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
        "    :param idx_to_word: vocabulary map (index -> word) (dict) -- added by us\n",
        "    :param oov_terms: list of OOV terms (list)\n",
        "    :param co_occorruence_count_matrix: the co-occurrence count matrix of the given dataset (window size 1)\n",
        "\n",
        "    :return\n",
        "        - embedding matrix that assigns a high dimensional vector to each word in the dataset specific vocabulary (shape |V| x d)\n",
        "    \"\"\"    \n",
        "    adj_oov = {} # --> oov : [adjacent terms]\n",
        "    oov_embedded = {} # --> oov : [adj terms mean]\n",
        "\n",
        "    def get_embedded_word(token):\n",
        "      if token in embedding_model:\n",
        "        return np.array(embedding_model[token])\n",
        "      else:\n",
        "        return np.array(oov_embedded[token])\n",
        "\n",
        "    #############################\n",
        "    # extracting adjacent words #\n",
        "    #############################\n",
        "    for oov_term in oov_terms:\n",
        "      oov_index = word_to_idx[oov_term]\n",
        "      adjacent_indices = co_occurrence_count_matrix.getrow(oov_index).nonzero()[1]\n",
        "      adjacent_terms = list(map(idx_to_word.get, filter(lambda token : idx_to_word[token] not in oov_terms, adjacent_indices)))\n",
        "      adj_oov.update({oov_term : adjacent_terms})\n",
        "    #############################\n",
        "    # neighbours' mean ##########\n",
        "    #############################\n",
        "    for oov_term, adj_terms in adj_oov.items():\n",
        "      if len(adj_terms) is 0:\n",
        "        # the list of adj words is empty --> assign a rndm vector\n",
        "        adj_terms_mean = np.random.rand(embedding_dimension)\n",
        "      else :\n",
        "        # compute the mean of adj terms\n",
        "        embedded_adj_terms = [embedding_model[term] for term in adj_terms]\n",
        "        adj_terms_mean = np.mean(embedded_adj_terms, axis=0)\n",
        "      oov_embedded.update({oov_term : adj_terms_mean})\n",
        "    embedding_model.add(list(filter(lambda token : token in embedding_model or token in oov_embedded, word_to_idx.keys())), \n",
        "                    list(map(get_embedded_word, filter(lambda token : token in embedding_model or token in oov_embedded, word_to_idx.keys()))))\n",
        "    return embedding_model "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcKpDgFRPnzr"
      },
      "source": [
        "#Preparing the sets to the usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVut30KhPxPE"
      },
      "source": [
        "##Train set:\n",
        "- X\n",
        "  - Building vocabulary\n",
        "  - Encoding sequences\n",
        "  - Padding\n",
        "  - Building embedded matrix\n",
        "- Y\n",
        "  - One hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ysp734xtDfnz"
      },
      "source": [
        "\n",
        "#creating dictionary and embedding Train\n",
        "X_train_idx_to_word, X_train_word_to_idx, X_train_word_listing, X_train_tokenizer= build_vocabulary(X_train)\n",
        "Y_train_idx_to_word, Y_train_word_to_idx, Y_train_word_listing, Y_train_tokenizer= build_vocabulary(Y_train)\n",
        "\n",
        "X_train_encoded = X_train_tokenizer.texts_to_sequences(X_train)\n",
        "Y_train_encoded = Y_train_tokenizer.texts_to_sequences(Y_train)\n",
        "\n",
        "X_train_padded = pad_sequences(X_train_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"post\", truncating=\"post\")\n",
        "Y_train_padded = pad_sequences(Y_train_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"post\", truncating=\"post\")\n",
        "\n",
        "X_train_oov = check_OOV_terms(embedding_model, X_train_word_listing)\n",
        "X_train_co_oc = co_occurrence_count(X_train, X_train_idx_to_word, X_train_word_to_idx, window_size=4)\n",
        "\n",
        "embedding_matrix = build_embedding_matrix(embedding_model, embedding_dimension, X_train_word_to_idx, X_train_idx_to_word, X_train_oov, X_train_co_oc)\n",
        "\n",
        "X_train_embedded = np.zeros((*X_train_padded.shape,EMBEDDING_SIZE ))\n",
        "for idx_line, line in enumerate(X_train_padded):\n",
        "  for idx_word, vocab_idx in enumerate(line):\n",
        "    if(vocab_idx != 0):\n",
        "      X_train_embedded[idx_line,idx_word,:] = embedding_matrix[X_train_idx_to_word[vocab_idx]]\n",
        "\n",
        "Y_train_ohe = to_categorical(Y_train_padded, num_classes= 46 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhRXgQIlQF1S"
      },
      "source": [
        "##Validation set:\n",
        "- X\n",
        "  - Building vocabulary\n",
        "  - Encoding sequences\n",
        "  - Padding\n",
        "  - Building embedded matrix\n",
        "- Y\n",
        "  - One hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pc7HOT92IKLN"
      },
      "source": [
        "#creating dictionary and embedding val\n",
        "X_val_idx_to_word, X_val_word_to_idx, X_val_word_listing, X_val_tokenizer= build_vocabulary(X_val)\n",
        "\n",
        "X_val_encoded = X_val_tokenizer.texts_to_sequences(X_val)\n",
        "Y_val_encoded = Y_train_tokenizer.texts_to_sequences(Y_val)\n",
        "\n",
        "X_val_padded = pad_sequences(X_val_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"post\", truncating=\"post\")\n",
        "Y_val_padded = pad_sequences(Y_val_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"post\", truncating=\"post\")\n",
        "\n",
        "X_val_oov = check_OOV_terms(embedding_matrix, X_val_word_listing)\n",
        "X_val_co_oc = co_occurrence_count(X_val, X_val_idx_to_word, X_val_word_to_idx, window_size=4)\n",
        "\n",
        "embedding_matrix = build_embedding_matrix(embedding_matrix, embedding_dimension, X_val_word_to_idx, X_val_idx_to_word, X_val_oov, X_val_co_oc)\n",
        "\n",
        "X_val_embedded = np.zeros((*X_val_padded.shape,EMBEDDING_SIZE ))\n",
        "for idx_line, line in enumerate(X_val_padded):\n",
        "  for idx_word, vocab_idx in enumerate(line):\n",
        "    if(vocab_idx != 0):\n",
        "      X_val_embedded[idx_line,idx_word,:] = embedding_matrix[X_val_idx_to_word[vocab_idx]]\n",
        "\n",
        "Y_val_ohe = to_categorical(Y_val_padded, num_classes= 46 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTTQOXpgQKf3"
      },
      "source": [
        "##Test set:\n",
        "- X\n",
        "  - Building vocabulary\n",
        "  - Encoding sequences\n",
        "  - Padding\n",
        "  - Building embedded matrix\n",
        "- Y\n",
        "  - One hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpQkoOYoINJi"
      },
      "source": [
        "#creating dictionary and embedding test\n",
        "X_test_idx_to_word, X_test_word_to_idx, X_test_word_listing, X_test_tokenizer= build_vocabulary(X_test)\n",
        "\n",
        "X_test_encoded = X_test_tokenizer.texts_to_sequences(X_test)\n",
        "Y_test_encoded = Y_train_tokenizer.texts_to_sequences(Y_test)\n",
        "\n",
        "X_test_padded = pad_sequences(X_test_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"post\", truncating=\"post\")\n",
        "Y_test_padded = pad_sequences(Y_test_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"post\", truncating=\"post\")\n",
        "\n",
        "X_test_oov = check_OOV_terms(embedding_matrix, X_test_word_listing)\n",
        "X_test_co_oc = co_occurrence_count(X_test, X_test_idx_to_word, X_test_word_to_idx, window_size=4)\n",
        "\n",
        "embedding_matrix = build_embedding_matrix(embedding_matrix, embedding_dimension, X_test_word_to_idx, X_test_idx_to_word, X_test_oov, X_test_co_oc)\n",
        "\n",
        "X_test_embedded = np.zeros((*X_test_padded.shape,EMBEDDING_SIZE ))\n",
        "for idx_line, line in enumerate(X_test_padded):\n",
        "  for idx_word, vocab_idx in enumerate(line):\n",
        "    if(vocab_idx != 0):\n",
        "      X_test_embedded[idx_line,idx_word,:] = embedding_matrix[X_test_idx_to_word[vocab_idx]]\n",
        "\n",
        "Y_test_ohe = to_categorical(Y_test_padded, num_classes= 46 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpO8WDCJPY7B"
      },
      "source": [
        "##Setting global variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neurCBlKPYW6"
      },
      "source": [
        "BATCH_SIZE = 20\n",
        "EPOCHS = 20\n",
        "VOCABULARY_SIZE = len(X_train_tokenizer.word_index) + 1\n",
        "NUM_CLASSES = len(Y_train_word_listing) + 1\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
        "Y_test_ohe = to_categorical(Y_test_padded, num_classes = 46)\n",
        "Y_trai_ohe = to_categorical(Y_train_padded, num_classes = 46)\n",
        "Y_val_ohe = to_categorical(Y_val_padded, num_classes = 46)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScJrkwCOQS4B"
      },
      "source": [
        "#Building and training models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYRzbeE1VLOr"
      },
      "source": [
        "TODO: ci sono due modell che mi paion uguali bisogna capire se uno è da eliminare (probabilmente si)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "go1NSZKoQXF5"
      },
      "source": [
        "##Bidirecitonal LSTM with fully connected layer "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLMroUcERnJa"
      },
      "source": [
        "###Building model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqzrTBNb4GV8"
      },
      "source": [
        "#BiLSTM + FC\n",
        "# create architecture\n",
        "BiLSTMFC = Sequential()\n",
        "\n",
        "# create embedding layer — usually the first layer in text problems\n",
        "# vocabulary size — number of unique words in data\n",
        "BiLSTMFC.add(Masking(mask_value=0, input_shape=(MAX_SEQ_LENGTH , 50)))\n",
        "\n",
        "# add an RNN layer which contains 64 RNN cells\n",
        "# True — return whole sequence; False — return single output of the end of the sequence\n",
        "BiLSTMFC.add(Bidirectional(LSTM(64, dropout = 0.2,  return_sequences=True)))\n",
        "\n",
        "# add time distributed (output at each sequence) layer\n",
        "BiLSTMFC.add(TimeDistributed(Dense(NUM_CLASSES, activation='softmax')))\n",
        "#compile model\n",
        "BiLSTMFC.compile(loss      =  'categorical_crossentropy',\n",
        "                  optimizer =  'adam',\n",
        "                  metrics   =  ['acc'])\n",
        "# check summary of the model\n",
        "BiLSTMFC.summary()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1NbMWyDRhwW"
      },
      "source": [
        "### Training model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-Sod5YeSRYQM"
      },
      "source": [
        "historyBiLSTMFC = BiLSTMFC.fit(X_train_embedded, Y_train_ohe, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(X_val_embedded, Y_val_ohe), callbacks=[early_stop])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S9hs2mqRbS_"
      },
      "source": [
        "###Graphical view of the model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gfq9sNPwRc_f"
      },
      "source": [
        "plt.plot(historyBiLSTMFC.history['acc'])  \n",
        "plt.plot(historyBiLSTMFC.history['loss'])  \n",
        "plt.plot(historyBiLSTMFC.history['val_acc'])  \n",
        "plt.plot(historyBiLSTMFC.history['val_loss']) \n",
        "plt.axis([0,25,0,1]) \n",
        "plt.title('model accuracy')  \n",
        "plt.ylabel('accuracy')  \n",
        "plt.xlabel('epoch')  \n",
        "plt.legend(['accuracy', 'loss','val_accuracy', 'val_loss'], loc='best') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfU5JJ1uRv3S"
      },
      "source": [
        "##Bidirectional GRU with fully connected layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mDK98NrR1zf"
      },
      "source": [
        "### Building model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbsrNY874XYW"
      },
      "source": [
        "#GRU + FC\n",
        "# create architecture\n",
        "BiGRUFC = Sequential()\n",
        "\n",
        "BiGRUFC.add(Masking(mask_value=0,input_shape=(MAX_SEQ_LENGTH , 50)))\n",
        "\n",
        "# add an RNN layer which contains 64 RNN cells\n",
        "# True — return whole sequence; False — return single output of the end of the sequence\n",
        "BiGRUFC.add(Bidirectional(GRU(64, dropout = 0.2, recurrent_dropout = 0.2, return_sequences=True)))\n",
        "\n",
        "# add time distributed (output at each sequence) layer\n",
        "BiGRUFC.add(TimeDistributed(Dense(NUM_CLASSES, activation='sigmoid')))\n",
        "\n",
        "#compile model\n",
        "BiGRUFC.compile(loss      =  'categorical_crossentropy',\n",
        "                  optimizer =  'adam',\n",
        "                  metrics   =  ['acc'])\n",
        "\n",
        "# check summary of the model\n",
        "BiGRUFC.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHthH_tySCVh"
      },
      "source": [
        "### Training model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AyKNqvPSCVi"
      },
      "source": [
        "historyBiGRUFC = BiGRUFC.fit(X_train_embedded, Y_train_ohe, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(X_val_embedded, Y_val_ohe), callbacks=[early_stop])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVbmLPf9SG_s"
      },
      "source": [
        "###Graphical view of the model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4mJ0iqMSG_s"
      },
      "source": [
        "plt.plot(historyBiGRUFC.history['acc'])  \n",
        "plt.plot(historyBiGRUFC.history['loss'])  \n",
        "plt.plot(historyBiGRUFC.history['val_acc'])  \n",
        "plt.plot(historyBiGRUFC.history['val_loss']) \n",
        "plt.axis([0,20-1,0,1]) \n",
        "plt.title('model accuracy')  \n",
        "plt.ylabel('accuracy')  \n",
        "plt.xlabel('epoch')  \n",
        "plt.legend(['accuracy', 'loss','val_accuracy', 'val_loss'], loc='best') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5y6UWK9pSRDd"
      },
      "source": [
        "## Double bidirectional LSTM with fully connected layer "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeTLLkqzSgwS"
      },
      "source": [
        "### Building model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqjPQFP6-9U1"
      },
      "source": [
        "#BiLSTM + BiLSTM + FC\n",
        "# create architecture\n",
        "BiBiLSTMFC = Sequential()\n",
        "\n",
        "# create embedding layer — usually the first layer in text problems\n",
        "# vocabulary size — number of unique words in data\n",
        "BiBiLSTMFC.add(Masking(mask_value=0, input_shape=(MAX_SEQ_LENGTH , 50)))\n",
        "\n",
        "# add an RNN layer which contains 64 RNN cells\n",
        "# True — return whole sequence; False — return single output of the end of the sequence\n",
        "BiBiLSTMFC.add(Bidirectional(LSTM(64, recurrent_dropout = 0.2, return_sequences=True)))\n",
        "\n",
        "BiBiLSTMFC.add(Bidirectional(LSTM(64, dropout = 0.2, recurrent_dropout = 0.2, return_sequences=True)))\n",
        "\n",
        "# add time distributed (output at each sequence) layer\n",
        "BiBiLSTMFC.add(TimeDistributed(Dense(NUM_CLASSES, activation='softmax')))\n",
        "#compile model\n",
        "BiBiLSTMFC.compile(loss      =  'categorical_crossentropy',\n",
        "                  optimizer =  'adam',\n",
        "                  metrics   =  ['acc'])\n",
        "# check summary of the model\n",
        "BiBiLSTMFC.summary()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOJ9HOsWTDBp"
      },
      "source": [
        "###Training model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQB5Q2Q2TcHK"
      },
      "source": [
        "historyBiBiLSTMFC = BiBiLSTMFC.fit(X_train_embedded, Y_train_ohe, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(X_val_embedded, Y_val_ohe), callbacks=[early_stop])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9_0ViLCTG78"
      },
      "source": [
        "### Graphical view of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDSa57tbTdQ7"
      },
      "source": [
        "plt.plot(historyBiBiLSTMFC.history['acc'])  \n",
        "plt.plot(historyBiBiLSTMFC.history['loss'])  \n",
        "plt.plot(historyBiBiLSTMFC.history['val_acc'])  \n",
        "plt.plot(historyBiBiLSTMFC.history['val_loss']) \n",
        "plt.axis([0,20-1,0,1]) \n",
        "plt.title('model accuracy')  \n",
        "plt.ylabel('accuracy')  \n",
        "plt.xlabel('epoch')  \n",
        "plt.legend(['accuracy', 'loss','val_accuracy', 'val_loss'], loc='best') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uO4ASIkS9eC"
      },
      "source": [
        "##Bidirectional LSTM with fully connected layers and CRF\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D359Gu9RWbTK"
      },
      "source": [
        "### Building model\n",
        "https://machinelearningmastery.com/keras-functional-api-deep-learning \\\\\n",
        "questo link ha un ottima spiegazione del perchè la costruzione è diversa dal solito"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hioa4QswWSgr"
      },
      "source": [
        "import tensorflow_addons as tfa\n",
        "from tf2crf import CRF, ModelWithCRFLoss\n",
        "from keras.layers import Input \n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "inputs = Input(shape=(MAX_SEQ_LENGTH , 50), dtype='float32')\n",
        "output = Masking(mask_value=np.zeros((MAX_SEQ_LENGTH , 50), dtype=\"float32\"))(inputs)\n",
        "output = Bidirectional(LSTM(64, return_sequences = True, dropout=0.4))(output)\n",
        "output = Dense(NUM_CLASSES+1, activation=None)(output)\n",
        "crf = CRF(dtype='float32')\n",
        "output = crf(output)\n",
        "BiLSTMFCCRF = Model(inputs, output)\n",
        "\n",
        "print(BiLSTMFCCRF.summary(line_length=150))\n",
        "\n",
        "BiLSTMFCCRF = ModelWithCRFLoss(BiLSTMFCCRF)\n",
        "\n",
        "BiLSTMFCCRF.compile(optimizer =  'adam',\n",
        "                 metrics   =  ['acc'])\n",
        "print(X_train_embedded.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bu4EQsc3ZiaE"
      },
      "source": [
        "### Training model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZkc3SRgZonc"
      },
      "source": [
        "historyBiLSTMFCCRF = BiLSTMFCCRF.fit(X_train_embedded, Y_train_padded, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(X_val_embedded, Y_val_padded))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLxC8ES6bcG-"
      },
      "source": [
        "### Graphical view of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecYlmLklbcG-"
      },
      "source": [
        "plt.plot(historyBiLSTMFCCRF.history['val_crf_loss_val'])  \n",
        "plt.plot(historyBiLSTMFCCRF.history['crf_loss'])  \n",
        "plt.axis([0,20-1,0,50]) \n",
        "plt.title('model accuracy')  \n",
        "plt.ylabel('accuracy')  \n",
        "plt.xlabel('epoch')  \n",
        "plt.legend(['val_crf_loss_val', 'loss'], loc='best') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ispLphROb6hx"
      },
      "source": [
        "plt.plot(historyBiLSTMFCCRF.history['val_val_accuracy'])  \n",
        "plt.plot(historyBiLSTMFCCRF.history['accuracy']) \n",
        "plt.title('model accuracy')  \n",
        "plt.ylabel('accuracy')  \n",
        "plt.xlabel('epoch')  \n",
        "plt.legend(['val_accuracy', 'accuracy'], loc='best') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KehNfTISFDCP"
      },
      "source": [
        "# Conclusion \n",
        "## Evaluation of the choosen model\n",
        "For this section we choose the double bidirectional LSTM + FC model (BiBiLSTMFC), because it is the model that performs better with the given paramteres.\n",
        "\n",
        "In the following cells we are about to calculate the f1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTG3oXUsKO0-"
      },
      "source": [
        "y_predict = BiBiLSTMFC.predict(X_test_embedded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trL6JoG3rEgX"
      },
      "source": [
        "def depad(true, predicted):\n",
        "  depad_pred = []\n",
        "  depad_true = []\n",
        "  for t, p in zip(true, predicted):\n",
        "    if t != 0:\n",
        "      depad_pred.append(p)\n",
        "      depad_true.append(t)\n",
        "\n",
        "  return (depad_true, depad_pred)\n",
        "\n",
        "def depunctuate(true, predicted):\n",
        "  depunctuate_pred = []\n",
        "  depunctuate_true = []\n",
        "  symbols = [Y_train_word_to_idx[a] for a in [\"$\", \",\", \".\", \":\", \"#\"]]\n",
        "  for t, p in zip(true, predicted):\n",
        "    if t not in symbols:\n",
        "      depunctuate_pred.append(p)\n",
        "      depunctuate_true.append(t)\n",
        "  return (depunctuate_true, depunctuate_pred)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKP-vh11FijC"
      },
      "source": [
        "y_pred = []\n",
        "for a in y_predict:\n",
        "  inner_y_pred = []\n",
        "  for e in a:\n",
        "    inner_y_pred.append(np.argmax(e))\n",
        "  y_pred.append(inner_y_pred)\n",
        "y_pred = np.array([np.array(y) for y in y_pred])\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "f1_scores = []\n",
        "print(\"-\"*40)\n",
        "for (true, predicted) in zip(Y_test_padded, y_pred):\n",
        "  true, predicted = depad(true,predicted)\n",
        "  true, predicted = depunctuate(true,predicted)\n",
        "  if true:\n",
        "    f1_scores.append(f1_score(true, predicted, average='macro'))\n",
        "print(np.mean(f1_scores))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}